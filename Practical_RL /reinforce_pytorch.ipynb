{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "reinforce_pytorch.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/randy-tsukemen/Data_science_roadmap/blob/master/Practical_RL%20/reinforce_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gFmphF6WdGb",
        "colab_type": "text"
      },
      "source": [
        "# REINFORCE in PyTorch\n",
        "\n",
        "Just like we did before for Q-learning, this time we'll design a PyTorch network to learn `CartPole-v0` via policy gradient (REINFORCE).\n",
        "\n",
        "Most of the code in this notebook is taken from approximate Q-learning, so you'll find it more or less familiar and even simpler."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMR2gD_-WdGd",
        "colab_type": "code",
        "outputId": "cda0dc80-4777-43ab-dd1c-5188adb84762",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "import sys, os\n",
        "if 'google.colab' in sys.modules and not os.path.exists('.setup_complete'):\n",
        "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/spring20/setup_colab.sh -O- | bash\n",
        "    !touch .setup_complete\n",
        "\n",
        "# This code creates a virtual display to draw game images on.\n",
        "# It will have no effect if your machine has a monitor.\n",
        "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
        "    !bash ../xvfb start\n",
        "    os.environ['DISPLAY'] = ':1'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Selecting previously unselected package xvfb.\n",
            "(Reading database ... 144433 files and directories currently installed.)\n",
            "Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.4_amd64.deb ...\n",
            "Unpacking xvfb (2:1.19.6-1ubuntu4.4) ...\n",
            "Setting up xvfb (2:1.19.6-1ubuntu4.4) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Starting virtual X frame buffer: Xvfb.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ofr1ce9TWdGp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0Lt5XS0WdG3",
        "colab_type": "text"
      },
      "source": [
        "A caveat: we have received reports that the following cell may crash with `NameError: name 'base' is not defined`. The [suggested workaround](https://www.coursera.org/learn/practical-rl/discussions/all/threads/N2Pw652iEemRYQ6W2GuqHg/replies/te3HpQwOQ62tx6UMDoOt2Q/comments/o08gTqelT9KPIE6npX_S3A) is to install `gym==0.14.0` and `pyglet==1.3.2`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9EMNmiwWdG5",
        "colab_type": "code",
        "outputId": "64bdaac7-90ae-4370-9e23-66e4fecbbe98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        }
      },
      "source": [
        "env = gym.make(\"CartPole-v0\")\n",
        "\n",
        "# gym compatibility: unwrap TimeLimit\n",
        "if hasattr(env, '_max_episode_steps'):\n",
        "    env = env.env\n",
        "\n",
        "env.reset()\n",
        "n_actions = env.action_space.n\n",
        "state_dim = env.observation_space.shape\n",
        "\n",
        "plt.imshow(env.render(\"rgb_array\"))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fbc8883ee80>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avud7vFMWdHL",
        "colab_type": "text"
      },
      "source": [
        "# Building the network for REINFORCE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuIxEWevWdHL",
        "colab_type": "text"
      },
      "source": [
        "For REINFORCE algorithm, we'll need a model that predicts action probabilities given states.\n",
        "\n",
        "For numerical stability, please __do not include the softmax layer into your network architecture__.\n",
        "We'll use softmax or log-softmax where appropriate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bsKNoarWWdHN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVxqQg7mW7so",
        "colab_type": "code",
        "outputId": "573354ab-b5af-4cbc-f74c-7f2ba11b937b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "state_dim"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mmc5QsgPXHHE",
        "colab_type": "code",
        "outputId": "a2c2fbe3-aef5-475e-e7c6-20ad03a59129",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "n_actions"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNNQcvg2WdHV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build a simple neural network that predicts policy logits. \n",
        "# Keep it simple: CartPole isn't worth deep architectures.\n",
        "model = nn.Sequential(\n",
        "  nn.Linear(state_dim[0], 2),\n",
        "  nn.ReLU(),\n",
        "  nn.Linear(2, n_actions)\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gbfK_KQJm0i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "05f52e3a-207a-4588-f959-c7b68be5996b"
      },
      "source": [
        "model"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Linear(in_features=4, out_features=2, bias=True)\n",
              "  (1): ReLU()\n",
              "  (2): Linear(in_features=2, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgdUR_tkWdHe",
        "colab_type": "text"
      },
      "source": [
        "#### Predict function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ccN_C-jWdHg",
        "colab_type": "text"
      },
      "source": [
        "Note: output value of this function is not a torch tensor, it's a numpy array.\n",
        "So, here gradient calculation is not needed.\n",
        "<br>\n",
        "Use [no_grad](https://pytorch.org/docs/stable/autograd.html#torch.autograd.no_grad)\n",
        "to suppress gradient calculation.\n",
        "<br>\n",
        "Also, `.detach()` (or legacy `.data` property) can be used instead, but there is a difference:\n",
        "<br>\n",
        "With `.detach()` computational graph is built but then disconnected from a particular tensor,\n",
        "so `.detach()` should be used if that graph is needed for backprop via some other (not detached) tensor;\n",
        "<br>\n",
        "In contrast, no graph is built by any operation in `no_grad()` context, thus it's preferable here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QO8d92paWdHj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_probs(states):\n",
        "    \"\"\" \n",
        "    Predict action probabilities given states.\n",
        "    :param states: numpy array of shape [batch, state_shape]\n",
        "    :returns: numpy array of shape [batch, n_actions]\n",
        "    \"\"\"\n",
        "    # convert states, compute logits, use softmax to get probability\n",
        "    with torch.no_grad():\n",
        "        states = torch.tensor(states, dtype=torch.float)\n",
        "        x = nn.functional.softmax(model(states), dim=1).numpy()\n",
        "\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vO2LTZqWdHq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_states = np.array([env.reset() for _ in range(5)])\n",
        "test_probas = predict_probs(test_states)\n",
        "assert isinstance(test_probas, np.ndarray), \\\n",
        "    \"you must return np array and not %s\" % type(test_probas)\n",
        "assert tuple(test_probas.shape) == (test_states.shape[0], env.action_space.n), \\\n",
        "    \"wrong output shape: %s\" % np.shape(test_probas)\n",
        "assert np.allclose(np.sum(test_probas, axis=1), 1), \"probabilities do not sum to 1\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02u_6buTJfHh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "b62ccd17-9874-4e7d-f573-83ab934738b1"
      },
      "source": [
        "test_probas"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.6821497 , 0.3178503 ],\n",
              "       [0.68223536, 0.31776464],\n",
              "       [0.68234354, 0.3176565 ],\n",
              "       [0.6825578 , 0.31744212],\n",
              "       [0.6820165 , 0.31798345]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rSzQWPaWdHv",
        "colab_type": "text"
      },
      "source": [
        "### Play the game\n",
        "\n",
        "We can now use our newly built agent to play the game."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2Se52gkWdHw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_session(env, t_max=1000):\n",
        "    \"\"\" \n",
        "    Play a full session with REINFORCE agent.\n",
        "    Returns sequences of states, actions, and rewards.\n",
        "    \"\"\"\n",
        "    # arrays to record session\n",
        "    states, actions, rewards = [], [], []\n",
        "    s = env.reset()\n",
        "\n",
        "    for t in range(t_max):\n",
        "        # action probabilities array aka pi(a|s)\n",
        "        action_probs = predict_probs(np.array([s]))[0]\n",
        "\n",
        "        # Sample action with given probabilities.\n",
        "        a = <YOUR CODE>\n",
        "        new_s, r, done, info = env.step(a)\n",
        "\n",
        "        # record session history to train later\n",
        "        states.append(s)\n",
        "        actions.append(a)\n",
        "        rewards.append(r)\n",
        "\n",
        "        s = new_s\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    return states, actions, rewards"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MoHSemw2WdH4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test it\n",
        "states, actions, rewards = generate_session(env)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQFO4v4YWdH-",
        "colab_type": "text"
      },
      "source": [
        "### Computing cumulative rewards\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "G_t &= r_t + \\gamma r_{t + 1} + \\gamma^2 r_{t + 2} + \\ldots \\\\\n",
        "&= \\sum_{i = t}^T \\gamma^{i - t} r_i \\\\\n",
        "&= r_t + \\gamma * G_{t + 1}\n",
        "\\end{align*}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsru2WCvWdH_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_cumulative_rewards(rewards,  # rewards at each step\n",
        "                           gamma=0.99  # discount for reward\n",
        "                           ):\n",
        "    \"\"\"\n",
        "    Take a list of immediate rewards r(s,a) for the whole session \n",
        "    and compute cumulative returns (a.k.a. G(s,a) in Sutton '16).\n",
        "    \n",
        "    G_t = r_t + gamma*r_{t+1} + gamma^2*r_{t+2} + ...\n",
        "\n",
        "    A simple way to compute cumulative rewards is to iterate from the last\n",
        "    to the first timestep and compute G_t = r_t + gamma*G_{t+1} recurrently\n",
        "\n",
        "    You must return an array/list of cumulative rewards with as many elements as in the initial rewards.\n",
        "    \"\"\"\n",
        "    <YOUR CODE>\n",
        "    return <YOUR CODE: array of cumulative rewards>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_BBrqOYWdIG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "get_cumulative_rewards(rewards)\n",
        "assert len(get_cumulative_rewards(list(range(100)))) == 100\n",
        "assert np.allclose(\n",
        "    get_cumulative_rewards([0, 0, 1, 0, 0, 1, 0], gamma=0.9),\n",
        "    [1.40049, 1.5561, 1.729, 0.81, 0.9, 1.0, 0.0])\n",
        "assert np.allclose(\n",
        "    get_cumulative_rewards([0, 0, 1, -2, 3, -4, 0], gamma=0.5),\n",
        "    [0.0625, 0.125, 0.25, -1.5, 1.0, -4.0, 0.0])\n",
        "assert np.allclose(\n",
        "    get_cumulative_rewards([0, 0, 1, 2, 3, 4, 0], gamma=0),\n",
        "    [0, 0, 1, 2, 3, 4, 0])\n",
        "print(\"looks good!\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bA19F9twWdIO",
        "colab_type": "text"
      },
      "source": [
        "#### Loss function and updates\n",
        "\n",
        "We now need to define objective and update over policy gradient.\n",
        "\n",
        "Our objective function is\n",
        "\n",
        "$$ J \\approx  { 1 \\over N } \\sum_{s_i,a_i} G(s_i,a_i) $$\n",
        "\n",
        "REINFORCE defines a way to compute the gradient of the expected reward with respect to policy parameters. The formula is as follows:\n",
        "\n",
        "$$ \\nabla_\\theta \\hat J(\\theta) \\approx { 1 \\over N } \\sum_{s_i, a_i} \\nabla_\\theta \\log \\pi_\\theta (a_i \\mid s_i) \\cdot G_t(s_i, a_i) $$\n",
        "\n",
        "We can abuse PyTorch's capabilities for automatic differentiation by defining our objective function as follows:\n",
        "\n",
        "$$ \\hat J(\\theta) \\approx { 1 \\over N } \\sum_{s_i, a_i} \\log \\pi_\\theta (a_i \\mid s_i) \\cdot G_t(s_i, a_i) $$\n",
        "\n",
        "When you compute the gradient of that function with respect to network weights $\\theta$, it will become exactly the policy gradient."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhamV7ceWdIP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def to_one_hot(y_tensor, ndims):\n",
        "    \"\"\" helper: take an integer vector and convert it to 1-hot matrix. \"\"\"\n",
        "    y_tensor = y_tensor.type(torch.LongTensor).view(-1, 1)\n",
        "    y_one_hot = torch.zeros(\n",
        "        y_tensor.size()[0], ndims).scatter_(1, y_tensor, 1)\n",
        "    return y_one_hot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVmfveeZWdIU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Your code: define optimizers\n",
        "optimizer = torch.optim.Adam(model.parameters(), 1e-3)\n",
        "\n",
        "\n",
        "def train_on_session(states, actions, rewards, gamma=0.99, entropy_coef=1e-2):\n",
        "    \"\"\"\n",
        "    Takes a sequence of states, actions and rewards produced by generate_session.\n",
        "    Updates agent's weights by following the policy gradient above.\n",
        "    Please use Adam optimizer with default parameters.\n",
        "    \"\"\"\n",
        "\n",
        "    # cast everything into torch tensors\n",
        "    states = torch.tensor(states, dtype=torch.float32)\n",
        "    actions = torch.tensor(actions, dtype=torch.int32)\n",
        "    cumulative_returns = np.array(get_cumulative_rewards(rewards, gamma))\n",
        "    cumulative_returns = torch.tensor(cumulative_returns, dtype=torch.float32)\n",
        "\n",
        "    # predict logits, probas and log-probas using an agent.\n",
        "    logits = model(states)\n",
        "    probs = nn.functional.softmax(logits, -1)\n",
        "    log_probs = nn.functional.log_softmax(logits, -1)\n",
        "\n",
        "    assert all(isinstance(v, torch.Tensor) for v in [logits, probs, log_probs]), \\\n",
        "        \"please use compute using torch tensors and don't use predict_probs function\"\n",
        "\n",
        "    # select log-probabilities for chosen actions, log pi(a_i|s_i)\n",
        "    log_probs_for_actions = torch.sum(\n",
        "        log_probs * to_one_hot(actions, env.action_space.n), dim=1)\n",
        "   \n",
        "    # Compute loss here. Don't forgen entropy regularization with `entropy_coef` \n",
        "    entropy = <YOUR CODE>\n",
        "    loss = <YOUR CODE>\n",
        "\n",
        "    # Gradient descent step\n",
        "    <YOUR CODE>\n",
        "\n",
        "    # technical: return session rewards to print them later\n",
        "    return np.sum(rewards)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tWCMy1KWdIa",
        "colab_type": "text"
      },
      "source": [
        "### The actual training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzIjaa89WdIc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(100):\n",
        "    rewards = [train_on_session(*generate_session(env)) for _ in range(100)]  # generate new sessions\n",
        "    \n",
        "    print(\"mean reward:%.3f\" % (np.mean(rewards)))\n",
        "    \n",
        "    if np.mean(rewards) > 500:\n",
        "        print(\"You Win!\")  # but you can train even further\n",
        "        break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7zg8KZzWdIi",
        "colab_type": "text"
      },
      "source": [
        "### Results & video"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tr3TFpO_WdIk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Record sessions\n",
        "\n",
        "import gym.wrappers\n",
        "\n",
        "with gym.wrappers.Monitor(gym.make(\"CartPole-v0\"), directory=\"videos\", force=True) as env_monitor:\n",
        "    sessions = [generate_session(env_monitor) for _ in range(100)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAb8VsEaWdIo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Show video. This may not work in some setups. If it doesn't\n",
        "# work for you, you can download the videos and view them locally.\n",
        "\n",
        "from pathlib import Path\n",
        "from IPython.display import HTML\n",
        "\n",
        "video_names = sorted([s for s in Path('videos').iterdir() if s.suffix == '.mp4'])\n",
        "\n",
        "HTML(\"\"\"\n",
        "<video width=\"640\" height=\"480\" controls>\n",
        "  <source src=\"{}\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\".format(video_names[-1]))  # You can also try other indices"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}