{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "reinforce_pytorch.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/randy-tsukemen/Data_science_roadmap/blob/master/reinforce_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gFmphF6WdGb",
        "colab_type": "text"
      },
      "source": [
        "# REINFORCE in PyTorch\n",
        "\n",
        "Just like we did before for Q-learning, this time we'll design a PyTorch network to learn `CartPole-v0` via policy gradient (REINFORCE).\n",
        "\n",
        "Most of the code in this notebook is taken from approximate Q-learning, so you'll find it more or less familiar and even simpler."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMR2gD_-WdGd",
        "colab_type": "code",
        "outputId": "a895bc52-8004-4ddd-abf1-1b13609ff509",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "import sys, os\n",
        "if 'google.colab' in sys.modules and not os.path.exists('.setup_complete'):\n",
        "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/spring20/setup_colab.sh -O- | bash\n",
        "    !touch .setup_complete\n",
        "\n",
        "# This code creates a virtual display to draw game images on.\n",
        "# It will have no effect if your machine has a monitor.\n",
        "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
        "    !bash ../xvfb start\n",
        "    os.environ['DISPLAY'] = ':1'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Selecting previously unselected package xvfb.\n",
            "(Reading database ... 144439 files and directories currently installed.)\n",
            "Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.4_amd64.deb ...\n",
            "Unpacking xvfb (2:1.19.6-1ubuntu4.4) ...\n",
            "Setting up xvfb (2:1.19.6-1ubuntu4.4) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Starting virtual X frame buffer: Xvfb.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ofr1ce9TWdGp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0Lt5XS0WdG3",
        "colab_type": "text"
      },
      "source": [
        "A caveat: we have received reports that the following cell may crash with `NameError: name 'base' is not defined`. The [suggested workaround](https://www.coursera.org/learn/practical-rl/discussions/all/threads/N2Pw652iEemRYQ6W2GuqHg/replies/te3HpQwOQ62tx6UMDoOt2Q/comments/o08gTqelT9KPIE6npX_S3A) is to install `gym==0.14.0` and `pyglet==1.3.2`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9EMNmiwWdG5",
        "colab_type": "code",
        "outputId": "e71da032-d5ed-415f-d64d-0fc68abe5a2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        }
      },
      "source": [
        "env = gym.make(\"CartPole-v0\")\n",
        "\n",
        "# gym compatibility: unwrap TimeLimit\n",
        "if hasattr(env, '_max_episode_steps'):\n",
        "    env = env.env\n",
        "\n",
        "env.reset()\n",
        "n_actions = env.action_space.n\n",
        "state_dim = env.observation_space.shape\n",
        "\n",
        "plt.imshow(env.render(\"rgb_array\"))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f0958e5ed68>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avud7vFMWdHL",
        "colab_type": "text"
      },
      "source": [
        "# Building the network for REINFORCE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuIxEWevWdHL",
        "colab_type": "text"
      },
      "source": [
        "For REINFORCE algorithm, we'll need a model that predicts action probabilities given states.\n",
        "\n",
        "For numerical stability, please __do not include the softmax layer into your network architecture__.\n",
        "We'll use softmax or log-softmax where appropriate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bsKNoarWWdHN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVxqQg7mW7so",
        "colab_type": "code",
        "outputId": "f0ec1967-46fa-4aae-dd37-724d9a6663c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "state_dim"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mmc5QsgPXHHE",
        "colab_type": "code",
        "outputId": "c814b033-bb5d-47aa-fa6e-eec9803fdbdb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "n_actions"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNNQcvg2WdHV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build a simple neural network that predicts policy logits. \n",
        "# Keep it simple: CartPole isn't worth deep architectures.\n",
        "model = nn.Sequential(\n",
        "  nn.Linear(state_dim[0], 2),\n",
        "  nn.ReLU(),\n",
        "  nn.Linear(2, n_actions)\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gbfK_KQJm0i",
        "colab_type": "code",
        "outputId": "395b73a7-c822-4174-c809-0f8a29da36f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "model"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Linear(in_features=4, out_features=2, bias=True)\n",
              "  (1): ReLU()\n",
              "  (2): Linear(in_features=2, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgdUR_tkWdHe",
        "colab_type": "text"
      },
      "source": [
        "#### Predict function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ccN_C-jWdHg",
        "colab_type": "text"
      },
      "source": [
        "Note: output value of this function is not a torch tensor, it's a numpy array.\n",
        "So, here gradient calculation is not needed.\n",
        "<br>\n",
        "Use [no_grad](https://pytorch.org/docs/stable/autograd.html#torch.autograd.no_grad)\n",
        "to suppress gradient calculation.\n",
        "<br>\n",
        "Also, `.detach()` (or legacy `.data` property) can be used instead, but there is a difference:\n",
        "<br>\n",
        "With `.detach()` computational graph is built but then disconnected from a particular tensor,\n",
        "so `.detach()` should be used if that graph is needed for backprop via some other (not detached) tensor;\n",
        "<br>\n",
        "In contrast, no graph is built by any operation in `no_grad()` context, thus it's preferable here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QO8d92paWdHj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_probs(states):\n",
        "    \"\"\" \n",
        "    Predict action probabilities given states.\n",
        "    :param states: numpy array of shape [batch, state_shape]\n",
        "    :returns: numpy array of shape [batch, n_actions]\n",
        "    \"\"\"\n",
        "    # convert states, compute logits, use softmax to get probability\n",
        "    with torch.no_grad():\n",
        "        states = torch.tensor(states, dtype=torch.float)\n",
        "        x = nn.functional.softmax(model(states), dim=1).numpy()\n",
        "\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vO2LTZqWdHq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_states = np.array([env.reset() for _ in range(5)])\n",
        "test_probas = predict_probs(test_states)\n",
        "assert isinstance(test_probas, np.ndarray), \\\n",
        "    \"you must return np array and not %s\" % type(test_probas)\n",
        "assert tuple(test_probas.shape) == (test_states.shape[0], env.action_space.n), \\\n",
        "    \"wrong output shape: %s\" % np.shape(test_probas)\n",
        "assert np.allclose(np.sum(test_probas, axis=1), 1), \"probabilities do not sum to 1\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02u_6buTJfHh",
        "colab_type": "code",
        "outputId": "75f5136b-d4dd-45b5-cf40-a2162200de0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "test_probas"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.41004238, 0.5899576 ],\n",
              "       [0.41004238, 0.5899576 ],\n",
              "       [0.41004238, 0.5899576 ],\n",
              "       [0.41004238, 0.5899576 ],\n",
              "       [0.41004238, 0.5899576 ]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rSzQWPaWdHv",
        "colab_type": "text"
      },
      "source": [
        "### Play the game\n",
        "\n",
        "We can now use our newly built agent to play the game."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2Se52gkWdHw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_session(env, t_max=1000):\n",
        "    \"\"\" \n",
        "    Play a full session with REINFORCE agent.\n",
        "    Returns sequences of states, actions, and rewards.\n",
        "    \"\"\"\n",
        "    # arrays to record session\n",
        "    states, actions, rewards = [], [], []\n",
        "    s = env.reset()\n",
        "\n",
        "    for t in range(t_max):\n",
        "        # action probabilities array aka pi(a|s)\n",
        "        action_probs = predict_probs(np.array([s]))[0]\n",
        "\n",
        "        # Sample action with given probabilities.\n",
        "        a = np.random.choice([0, 1], p=action_probs)\n",
        "        new_s, r, done, info = env.step(a)\n",
        "\n",
        "        # record session history to train later\n",
        "        states.append(s)\n",
        "        actions.append(a)\n",
        "        rewards.append(r)\n",
        "\n",
        "        s = new_s\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    return states, actions, rewards"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MoHSemw2WdH4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test it\n",
        "states, actions, rewards = generate_session(env)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQFO4v4YWdH-",
        "colab_type": "text"
      },
      "source": [
        "### Computing cumulative rewards\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "G_t &= r_t + \\gamma r_{t + 1} + \\gamma^2 r_{t + 2} + \\ldots \\\\\n",
        "&= \\sum_{i = t}^T \\gamma^{i - t} r_i \\\\\n",
        "&= r_t + \\gamma * G_{t + 1}\n",
        "\\end{align*}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsru2WCvWdH_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_cumulative_rewards(rewards,  # rewards at each step\n",
        "                           gamma=0.99  # discount for reward\n",
        "                           ):\n",
        "    \"\"\"\n",
        "    Take a list of immediate rewards r(s,a) for the whole session \n",
        "    and compute cumulative returns (a.k.a. G(s,a) in Sutton '16).\n",
        "    \n",
        "    G_t = r_t + gamma*r_{t+1} + gamma^2*r_{t+2} + ...\n",
        "\n",
        "    A simple way to compute cumulative rewards is to iterate from the last\n",
        "    to the first timestep and compute G_t = r_t + gamma*G_{t+1} recurrently\n",
        "\n",
        "    You must return an array/list of cumulative rewards with as many elements as in the initial rewards.\n",
        "    \"\"\"\n",
        "    power = np.power(gamma, np.arange(len(rewards))).tolist()\n",
        "    cum_reward = np.correlate(np.array(rewards), power, 'full')[-len(rewards):].tolist()\n",
        "    return cum_reward"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_BBrqOYWdIG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2f3ccbe2-2420-4edf-edd1-9544dd495402"
      },
      "source": [
        "get_cumulative_rewards(rewards)\n",
        "assert len(get_cumulative_rewards(list(range(100)))) == 100\n",
        "assert np.allclose(\n",
        "    get_cumulative_rewards([0, 0, 1, 0, 0, 1, 0], gamma=0.9),\n",
        "    [1.40049, 1.5561, 1.729, 0.81, 0.9, 1.0, 0.0])\n",
        "assert np.allclose(\n",
        "    get_cumulative_rewards([0, 0, 1, -2, 3, -4, 0], gamma=0.5),\n",
        "    [0.0625, 0.125, 0.25, -1.5, 1.0, -4.0, 0.0])\n",
        "assert np.allclose(\n",
        "    get_cumulative_rewards([0, 0, 1, 2, 3, 4, 0], gamma=0),\n",
        "    [0, 0, 1, 2, 3, 4, 0])\n",
        "print(\"looks good!\")"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "looks good!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bA19F9twWdIO",
        "colab_type": "text"
      },
      "source": [
        "#### Loss function and updates\n",
        "\n",
        "We now need to define objective and update over policy gradient.\n",
        "\n",
        "Our objective function is\n",
        "\n",
        "$$ J \\approx  { 1 \\over N } \\sum_{s_i,a_i} G(s_i,a_i) $$\n",
        "\n",
        "REINFORCE defines a way to compute the gradient of the expected reward with respect to policy parameters. The formula is as follows:\n",
        "\n",
        "$$ \\nabla_\\theta \\hat J(\\theta) \\approx { 1 \\over N } \\sum_{s_i, a_i} \\nabla_\\theta \\log \\pi_\\theta (a_i \\mid s_i) \\cdot G_t(s_i, a_i) $$\n",
        "\n",
        "We can abuse PyTorch's capabilities for automatic differentiation by defining our objective function as follows:\n",
        "\n",
        "$$ \\hat J(\\theta) \\approx { 1 \\over N } \\sum_{s_i, a_i} \\log \\pi_\\theta (a_i \\mid s_i) \\cdot G_t(s_i, a_i) $$\n",
        "\n",
        "When you compute the gradient of that function with respect to network weights $\\theta$, it will become exactly the policy gradient."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhamV7ceWdIP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def to_one_hot(y_tensor, ndims):\n",
        "    \"\"\" helper: take an integer vector and convert it to 1-hot matrix. \"\"\"\n",
        "    y_tensor = y_tensor.type(torch.LongTensor).view(-1, 1)\n",
        "    y_one_hot = torch.zeros(\n",
        "        y_tensor.size()[0], ndims).scatter_(1, y_tensor, 1)\n",
        "    return y_one_hot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVmfveeZWdIU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Your code: define optimizers\n",
        "optimizer = torch.optim.Adam(model.parameters(), 1e-3)\n",
        "\n",
        "\n",
        "def train_on_session(states, actions, rewards, gamma=0.99, entropy_coef=1e-2):\n",
        "    \"\"\"\n",
        "    Takes a sequence of states, actions and rewards produced by generate_session.\n",
        "    Updates agent's weights by following the policy gradient above.\n",
        "    Please use Adam optimizer with default parameters.\n",
        "    \"\"\"\n",
        "\n",
        "    # cast everything into torch tensors\n",
        "    states = torch.tensor(states, dtype=torch.float32)\n",
        "    actions = torch.tensor(actions, dtype=torch.int32)\n",
        "    cumulative_returns = np.array(get_cumulative_rewards(rewards, gamma))\n",
        "    cumulative_returns = torch.tensor(cumulative_returns, dtype=torch.float32)\n",
        "\n",
        "    # predict logits, probas and log-probas using an agent.\n",
        "    logits = model(states)\n",
        "    probs = nn.functional.softmax(logits, -1)\n",
        "    log_probs = nn.functional.log_softmax(logits, -1)\n",
        "\n",
        "    assert all(isinstance(v, torch.Tensor) for v in [logits, probs, log_probs]), \\\n",
        "        \"please use compute using torch tensors and don't use predict_probs function\"\n",
        "\n",
        "    # select log-probabilities for chosen actions, log pi(a_i|s_i)\n",
        "    log_probs_for_actions = torch.sum(\n",
        "        log_probs * to_one_hot(actions, env.action_space.n), dim=1)\n",
        "   \n",
        "    # Compute loss here. Don't forgen entropy regularization with `entropy_coef` \n",
        "    entropy = <YOUR CODE>\n",
        "    loss = <YOUR CODE>\n",
        "\n",
        "    # Gradient descent step\n",
        "    <YOUR CODE>\n",
        "\n",
        "    # technical: return session rewards to print them later\n",
        "    return np.sum(rewards)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tWCMy1KWdIa",
        "colab_type": "text"
      },
      "source": [
        "### The actual training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzIjaa89WdIc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(100):\n",
        "    rewards = [train_on_session(*generate_session(env)) for _ in range(100)]  # generate new sessions\n",
        "    \n",
        "    print(\"mean reward:%.3f\" % (np.mean(rewards)))\n",
        "    \n",
        "    if np.mean(rewards) > 500:\n",
        "        print(\"You Win!\")  # but you can train even further\n",
        "        break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7zg8KZzWdIi",
        "colab_type": "text"
      },
      "source": [
        "### Results & video"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tr3TFpO_WdIk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Record sessions\n",
        "\n",
        "import gym.wrappers\n",
        "\n",
        "with gym.wrappers.Monitor(gym.make(\"CartPole-v0\"), directory=\"videos\", force=True) as env_monitor:\n",
        "    sessions = [generate_session(env_monitor) for _ in range(100)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAb8VsEaWdIo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Show video. This may not work in some setups. If it doesn't\n",
        "# work for you, you can download the videos and view them locally.\n",
        "\n",
        "from pathlib import Path\n",
        "from IPython.display import HTML\n",
        "\n",
        "video_names = sorted([s for s in Path('videos').iterdir() if s.suffix == '.mp4'])\n",
        "\n",
        "HTML(\"\"\"\n",
        "<video width=\"640\" height=\"480\" controls>\n",
        "  <source src=\"{}\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\".format(video_names[-1]))  # You can also try other indices"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}