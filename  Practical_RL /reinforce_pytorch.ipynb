{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "reinforce_pytorch.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/randy-tsukemen/Data_science_roadmap/blob/master/%20Practical_RL%20/reinforce_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gFmphF6WdGb",
        "colab_type": "text"
      },
      "source": [
        "# REINFORCE in PyTorch\n",
        "\n",
        "Just like we did before for Q-learning, this time we'll design a PyTorch network to learn `CartPole-v0` via policy gradient (REINFORCE).\n",
        "\n",
        "Most of the code in this notebook is taken from approximate Q-learning, so you'll find it more or less familiar and even simpler."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMR2gD_-WdGd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "4785bb98-2549-4c22-8bc8-8575c6ba9efa"
      },
      "source": [
        "import sys, os\n",
        "if 'google.colab' in sys.modules and not os.path.exists('.setup_complete'):\n",
        "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/spring20/setup_colab.sh -O- | bash\n",
        "    !touch .setup_complete\n",
        "\n",
        "# This code creates a virtual display to draw game images on.\n",
        "# It will have no effect if your machine has a monitor.\n",
        "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
        "    !bash ../xvfb start\n",
        "    os.environ['DISPLAY'] = ':1'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Selecting previously unselected package xvfb.\n",
            "(Reading database ... 144433 files and directories currently installed.)\n",
            "Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.4_amd64.deb ...\n",
            "Unpacking xvfb (2:1.19.6-1ubuntu4.4) ...\n",
            "Setting up xvfb (2:1.19.6-1ubuntu4.4) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Starting virtual X frame buffer: Xvfb.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ofr1ce9TWdGp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0Lt5XS0WdG3",
        "colab_type": "text"
      },
      "source": [
        "A caveat: we have received reports that the following cell may crash with `NameError: name 'base' is not defined`. The [suggested workaround](https://www.coursera.org/learn/practical-rl/discussions/all/threads/N2Pw652iEemRYQ6W2GuqHg/replies/te3HpQwOQ62tx6UMDoOt2Q/comments/o08gTqelT9KPIE6npX_S3A) is to install `gym==0.14.0` and `pyglet==1.3.2`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9EMNmiwWdG5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "outputId": "1fe72bd4-2c65-4c34-ed0b-f6243b169e24"
      },
      "source": [
        "env = gym.make(\"CartPole-v0\")\n",
        "\n",
        "# gym compatibility: unwrap TimeLimit\n",
        "if hasattr(env, '_max_episode_steps'):\n",
        "    env = env.env\n",
        "\n",
        "env.reset()\n",
        "n_actions = env.action_space.n\n",
        "state_dim = env.observation_space.shape\n",
        "\n",
        "plt.imshow(env.render(\"rgb_array\"))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f5cdf45ee80>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avud7vFMWdHL",
        "colab_type": "text"
      },
      "source": [
        "# Building the network for REINFORCE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuIxEWevWdHL",
        "colab_type": "text"
      },
      "source": [
        "For REINFORCE algorithm, we'll need a model that predicts action probabilities given states.\n",
        "\n",
        "For numerical stability, please __do not include the softmax layer into your network architecture__.\n",
        "We'll use softmax or log-softmax where appropriate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bsKNoarWWdHN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVxqQg7mW7so",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "821c3471-32c6-4702-8eac-33c6913e1f17"
      },
      "source": [
        "state_dim"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mmc5QsgPXHHE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0cb6d372-a469-4b99-8244-4fbc38aeff0e"
      },
      "source": [
        "n_actions"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNNQcvg2WdHV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build a simple neural network that predicts policy logits. \n",
        "# Keep it simple: CartPole isn't worth deep architectures.\n",
        "model = nn.Sequential(\n",
        "  nn.Linear(state_dim[0], 2),\n",
        "  nn.Linear(2, n_actions)\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgdUR_tkWdHe",
        "colab_type": "text"
      },
      "source": [
        "#### Predict function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ccN_C-jWdHg",
        "colab_type": "text"
      },
      "source": [
        "Note: output value of this function is not a torch tensor, it's a numpy array.\n",
        "So, here gradient calculation is not needed.\n",
        "<br>\n",
        "Use [no_grad](https://pytorch.org/docs/stable/autograd.html#torch.autograd.no_grad)\n",
        "to suppress gradient calculation.\n",
        "<br>\n",
        "Also, `.detach()` (or legacy `.data` property) can be used instead, but there is a difference:\n",
        "<br>\n",
        "With `.detach()` computational graph is built but then disconnected from a particular tensor,\n",
        "so `.detach()` should be used if that graph is needed for backprop via some other (not detached) tensor;\n",
        "<br>\n",
        "In contrast, no graph is built by any operation in `no_grad()` context, thus it's preferable here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QO8d92paWdHj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_probs(states):\n",
        "    \"\"\" \n",
        "    Predict action probabilities given states.\n",
        "    :param states: numpy array of shape [batch, state_shape]\n",
        "    :returns: numpy array of shape [batch, n_actions]\n",
        "    \"\"\"\n",
        "    # convert states, compute logits, use softmax to get probability\n",
        "    <YOUR CODE>\n",
        "    return <YOUR CODE>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vO2LTZqWdHq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_states = np.array([env.reset() for _ in range(5)])\n",
        "test_probas = predict_probs(test_states)\n",
        "assert isinstance(test_probas, np.ndarray), \\\n",
        "    \"you must return np array and not %s\" % type(test_probas)\n",
        "assert tuple(test_probas.shape) == (test_states.shape[0], env.action_space.n), \\\n",
        "    \"wrong output shape: %s\" % np.shape(test_probas)\n",
        "assert np.allclose(np.sum(test_probas, axis=1), 1), \"probabilities do not sum to 1\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rSzQWPaWdHv",
        "colab_type": "text"
      },
      "source": [
        "### Play the game\n",
        "\n",
        "We can now use our newly built agent to play the game."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2Se52gkWdHw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_session(env, t_max=1000):\n",
        "    \"\"\" \n",
        "    Play a full session with REINFORCE agent.\n",
        "    Returns sequences of states, actions, and rewards.\n",
        "    \"\"\"\n",
        "    # arrays to record session\n",
        "    states, actions, rewards = [], [], []\n",
        "    s = env.reset()\n",
        "\n",
        "    for t in range(t_max):\n",
        "        # action probabilities array aka pi(a|s)\n",
        "        action_probs = predict_probs(np.array([s]))[0]\n",
        "\n",
        "        # Sample action with given probabilities.\n",
        "        a = <YOUR CODE>\n",
        "        new_s, r, done, info = env.step(a)\n",
        "\n",
        "        # record session history to train later\n",
        "        states.append(s)\n",
        "        actions.append(a)\n",
        "        rewards.append(r)\n",
        "\n",
        "        s = new_s\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    return states, actions, rewards"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MoHSemw2WdH4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test it\n",
        "states, actions, rewards = generate_session(env)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQFO4v4YWdH-",
        "colab_type": "text"
      },
      "source": [
        "### Computing cumulative rewards\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "G_t &= r_t + \\gamma r_{t + 1} + \\gamma^2 r_{t + 2} + \\ldots \\\\\n",
        "&= \\sum_{i = t}^T \\gamma^{i - t} r_i \\\\\n",
        "&= r_t + \\gamma * G_{t + 1}\n",
        "\\end{align*}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsru2WCvWdH_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_cumulative_rewards(rewards,  # rewards at each step\n",
        "                           gamma=0.99  # discount for reward\n",
        "                           ):\n",
        "    \"\"\"\n",
        "    Take a list of immediate rewards r(s,a) for the whole session \n",
        "    and compute cumulative returns (a.k.a. G(s,a) in Sutton '16).\n",
        "    \n",
        "    G_t = r_t + gamma*r_{t+1} + gamma^2*r_{t+2} + ...\n",
        "\n",
        "    A simple way to compute cumulative rewards is to iterate from the last\n",
        "    to the first timestep and compute G_t = r_t + gamma*G_{t+1} recurrently\n",
        "\n",
        "    You must return an array/list of cumulative rewards with as many elements as in the initial rewards.\n",
        "    \"\"\"\n",
        "    <YOUR CODE>\n",
        "    return <YOUR CODE: array of cumulative rewards>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_BBrqOYWdIG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "get_cumulative_rewards(rewards)\n",
        "assert len(get_cumulative_rewards(list(range(100)))) == 100\n",
        "assert np.allclose(\n",
        "    get_cumulative_rewards([0, 0, 1, 0, 0, 1, 0], gamma=0.9),\n",
        "    [1.40049, 1.5561, 1.729, 0.81, 0.9, 1.0, 0.0])\n",
        "assert np.allclose(\n",
        "    get_cumulative_rewards([0, 0, 1, -2, 3, -4, 0], gamma=0.5),\n",
        "    [0.0625, 0.125, 0.25, -1.5, 1.0, -4.0, 0.0])\n",
        "assert np.allclose(\n",
        "    get_cumulative_rewards([0, 0, 1, 2, 3, 4, 0], gamma=0),\n",
        "    [0, 0, 1, 2, 3, 4, 0])\n",
        "print(\"looks good!\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bA19F9twWdIO",
        "colab_type": "text"
      },
      "source": [
        "#### Loss function and updates\n",
        "\n",
        "We now need to define objective and update over policy gradient.\n",
        "\n",
        "Our objective function is\n",
        "\n",
        "$$ J \\approx  { 1 \\over N } \\sum_{s_i,a_i} G(s_i,a_i) $$\n",
        "\n",
        "REINFORCE defines a way to compute the gradient of the expected reward with respect to policy parameters. The formula is as follows:\n",
        "\n",
        "$$ \\nabla_\\theta \\hat J(\\theta) \\approx { 1 \\over N } \\sum_{s_i, a_i} \\nabla_\\theta \\log \\pi_\\theta (a_i \\mid s_i) \\cdot G_t(s_i, a_i) $$\n",
        "\n",
        "We can abuse PyTorch's capabilities for automatic differentiation by defining our objective function as follows:\n",
        "\n",
        "$$ \\hat J(\\theta) \\approx { 1 \\over N } \\sum_{s_i, a_i} \\log \\pi_\\theta (a_i \\mid s_i) \\cdot G_t(s_i, a_i) $$\n",
        "\n",
        "When you compute the gradient of that function with respect to network weights $\\theta$, it will become exactly the policy gradient."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhamV7ceWdIP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def to_one_hot(y_tensor, ndims):\n",
        "    \"\"\" helper: take an integer vector and convert it to 1-hot matrix. \"\"\"\n",
        "    y_tensor = y_tensor.type(torch.LongTensor).view(-1, 1)\n",
        "    y_one_hot = torch.zeros(\n",
        "        y_tensor.size()[0], ndims).scatter_(1, y_tensor, 1)\n",
        "    return y_one_hot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVmfveeZWdIU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Your code: define optimizers\n",
        "optimizer = torch.optim.Adam(model.parameters(), 1e-3)\n",
        "\n",
        "\n",
        "def train_on_session(states, actions, rewards, gamma=0.99, entropy_coef=1e-2):\n",
        "    \"\"\"\n",
        "    Takes a sequence of states, actions and rewards produced by generate_session.\n",
        "    Updates agent's weights by following the policy gradient above.\n",
        "    Please use Adam optimizer with default parameters.\n",
        "    \"\"\"\n",
        "\n",
        "    # cast everything into torch tensors\n",
        "    states = torch.tensor(states, dtype=torch.float32)\n",
        "    actions = torch.tensor(actions, dtype=torch.int32)\n",
        "    cumulative_returns = np.array(get_cumulative_rewards(rewards, gamma))\n",
        "    cumulative_returns = torch.tensor(cumulative_returns, dtype=torch.float32)\n",
        "\n",
        "    # predict logits, probas and log-probas using an agent.\n",
        "    logits = model(states)\n",
        "    probs = nn.functional.softmax(logits, -1)\n",
        "    log_probs = nn.functional.log_softmax(logits, -1)\n",
        "\n",
        "    assert all(isinstance(v, torch.Tensor) for v in [logits, probs, log_probs]), \\\n",
        "        \"please use compute using torch tensors and don't use predict_probs function\"\n",
        "\n",
        "    # select log-probabilities for chosen actions, log pi(a_i|s_i)\n",
        "    log_probs_for_actions = torch.sum(\n",
        "        log_probs * to_one_hot(actions, env.action_space.n), dim=1)\n",
        "   \n",
        "    # Compute loss here. Don't forgen entropy regularization with `entropy_coef` \n",
        "    entropy = <YOUR CODE>\n",
        "    loss = <YOUR CODE>\n",
        "\n",
        "    # Gradient descent step\n",
        "    <YOUR CODE>\n",
        "\n",
        "    # technical: return session rewards to print them later\n",
        "    return np.sum(rewards)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tWCMy1KWdIa",
        "colab_type": "text"
      },
      "source": [
        "### The actual training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzIjaa89WdIc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(100):\n",
        "    rewards = [train_on_session(*generate_session(env)) for _ in range(100)]  # generate new sessions\n",
        "    \n",
        "    print(\"mean reward:%.3f\" % (np.mean(rewards)))\n",
        "    \n",
        "    if np.mean(rewards) > 500:\n",
        "        print(\"You Win!\")  # but you can train even further\n",
        "        break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7zg8KZzWdIi",
        "colab_type": "text"
      },
      "source": [
        "### Results & video"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tr3TFpO_WdIk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Record sessions\n",
        "\n",
        "import gym.wrappers\n",
        "\n",
        "with gym.wrappers.Monitor(gym.make(\"CartPole-v0\"), directory=\"videos\", force=True) as env_monitor:\n",
        "    sessions = [generate_session(env_monitor) for _ in range(100)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAb8VsEaWdIo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Show video. This may not work in some setups. If it doesn't\n",
        "# work for you, you can download the videos and view them locally.\n",
        "\n",
        "from pathlib import Path\n",
        "from IPython.display import HTML\n",
        "\n",
        "video_names = sorted([s for s in Path('videos').iterdir() if s.suffix == '.mp4'])\n",
        "\n",
        "HTML(\"\"\"\n",
        "<video width=\"640\" height=\"480\" controls>\n",
        "  <source src=\"{}\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\".format(video_names[-1]))  # You can also try other indices"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}